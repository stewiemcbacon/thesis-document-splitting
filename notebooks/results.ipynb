{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_hf = pd.read_csv('sim_scores/c1_headerfooter.csv', index_col = 0)\n",
    "c2_hf = pd.read_csv('sim_scores/c2_headerfooter.csv', index_col = 0)\n",
    "c1_d2v = pd.read_csv('sim_scores/d2v_skip_c1.csv', index_col = 0)\n",
    "c2_d2v = pd.read_csv('sim_scores/d2v_skip_c2.csv', index_col = 0)\n",
    "c1_tfidf = pd.read_csv('sim_scores/tfidf_skip_c1.csv', index_col = 0)\n",
    "c2_tfidf = pd.read_csv('sim_scores/tfidf_skip_c2.csv', index_col = 0)\n",
    "c1 = pd.read_csv('dfs/c1.csv', index_col = 0)\n",
    "c2 = pd.read_csv('dfs/c2.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = c1.merge(c1_hf.drop(columns=['label']), how='left', on=['file_name','page']).merge(c1_d2v.drop(columns=['text_y_cleaned','label']), how='left',on=['full_name','page']).merge(c1_tfidf.drop(columns=['text_y_cleaned','label']), how='left',on=['full_name','page'])\n",
    "c2 = c2.merge(c2_hf.drop(columns=['label']), how='left', on=['file_name','page']).merge(c2_d2v.drop(columns=['text_y_cleaned','label']), how='left',on=['full_name','page']).merge(c2_tfidf.drop(columns=['text_y_cleaned','label']), how='left',on=['full_name','page'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_format(y):\n",
    "    y[0] = 1\n",
    "    indices = [i for i, x in enumerate(y) if x == 1]+[len(y)-1]\n",
    "    result = []\n",
    "    for i in range(len(indices)):\n",
    "        if i != len(indices)-1:\n",
    "            result.append(indices[i+1] - indices[i])\n",
    "    result[-1]+=1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_index(split):\n",
    "    '''Turns a doc length vector like [1,2,1,3,3,5] into a dict with pagenumbers as keys and the set of all \n",
    "    pagenumbers in the same document as value.\n",
    "    This thus is an index which gives for every page its cluster.'''\n",
    "    l= sum(split)\n",
    "    pages= list(np.arange(l))\n",
    "    out = defaultdict(set)\n",
    "    for block_length in split:\n",
    "        block= pages[:block_length]\n",
    "        pages= pages[block_length:]\n",
    "        for page in block:\n",
    "            out[page]= set(block)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bcubed(truth,pred):\n",
    "    assert sum(truth)==sum(pred)  # same amount of pages\n",
    "    truth,pred = make_index(truth), make_index(pred)\n",
    "    \n",
    "    df  ={i:{'size':len(truth[i]),'P':0,'R':0,'F1':0} for i in truth}\n",
    "    for i in truth:\n",
    "        df[i]['P']= len(truth[i] & pred[i])/len(pred[i]) \n",
    "        df[i]['R']= len(truth[i] & pred[i])/len(truth[i])\n",
    "        df[i]['F1']= (2*df[i]['P']*df[i]['R'])/(df[i]['P']+df[i]['R'])\n",
    "    df= pd.DataFrame.from_dict(df, orient='index')\n",
    "    df.index_name='PageNr'\n",
    "    return  df\n",
    "\n",
    "\n",
    "def MeanBcubed(truth,pred):\n",
    "    assert sum(truth)==sum(pred)  # same amount of pages\n",
    "    return Bcubed(truth,pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from collections import defaultdict\n",
    "\n",
    "_c1 = c1[~pd.isnull(c1['text_y_cleaned'])]\n",
    "_c2 = c2[~pd.isnull(c2['text_y_cleaned'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual features Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train C1, Predict C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "types = []\n",
    "trainings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "size    69.707824\n",
       "P        0.448520\n",
       "R        0.827935\n",
       "F1       0.369598\n",
       "dtype: float64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_features = ['font_diff3','crop_diff','header_diff','footer_diff']\n",
    "model = LogisticRegression()\n",
    "model.fit(_c1[vis_features], _c1['label'])\n",
    "\n",
    "X_test = _c2[vis_features]\n",
    "y_test = _c2['label']\n",
    "true = y_test\n",
    "preds = model.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(model.predict(X_test))\n",
    "\n",
    "res.append(MeanBcubed(vb_truth, vb_pred))\n",
    "types.append('visual only')\n",
    "trainings.append('c1c2')\n",
    "MeanBcubed(vb_truth, vb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train C2, Predict C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "size    34.403323\n",
       "P        0.369691\n",
       "R        0.942905\n",
       "F1       0.404325\n",
       "dtype: float64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_features = ['font_diff3','crop_diff','header_diff','footer_diff']\n",
    "model = LogisticRegression()\n",
    "model.fit(_c2[vis_features], _c2['label'])\n",
    "\n",
    "X_test = _c1[vis_features]\n",
    "y_test = _c1['label']\n",
    "true = y_test\n",
    "preds = model.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(model.predict(X_test))\n",
    "\n",
    "res.append(MeanBcubed(vb_truth, vb_pred))\n",
    "types.append('visual only')\n",
    "trainings.append('c2c1')\n",
    "MeanBcubed(vb_truth, vb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF + visual results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train C1, predict C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_features = ['font_diff3','crop_diff','header_diff','footer_diff','tfidf_sim']\n",
    "model = LogisticRegression()\n",
    "model.fit(_c1[vis_features], _c1['label'])\n",
    "\n",
    "X_test = _c2[vis_features]\n",
    "y_test = _c2['label']\n",
    "true = y_test\n",
    "preds = model.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(model.predict(X_test))\n",
    "MeanBcubed(vb_truth, vb_pred)\n",
    "res.append(MeanBcubed(vb_truth, vb_pred))\n",
    "types.append('visual + tfidf')\n",
    "trainings.append('c1c2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train C2, Predict C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "size    34.403323\n",
       "P        0.455809\n",
       "R        0.942125\n",
       "F1       0.490227\n",
       "dtype: float64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_features = ['font_diff3','crop_diff','header_diff','footer_diff', 'tfidf_sim']\n",
    "model = LogisticRegression()\n",
    "model.fit(_c2[vis_features], _c2['label'])\n",
    "\n",
    "X_test = _c1[vis_features]\n",
    "y_test = _c1['label']\n",
    "true = y_test\n",
    "preds = model.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(model.predict(X_test))\n",
    "res.append(MeanBcubed(vb_truth, vb_pred))\n",
    "types.append('visual + tfidf')\n",
    "trainings.append('c2c1')\n",
    "MeanBcubed(vb_truth, vb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D2V + Visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train C1, predict C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "size    69.707824\n",
       "P        0.559806\n",
       "R        0.768265\n",
       "F1       0.446680\n",
       "dtype: float64"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_features = ['font_diff3','crop_diff','header_diff','footer_diff','d2v_sim']\n",
    "model = LogisticRegression()\n",
    "model.fit(_c1[vis_features], _c1['label'])\n",
    "\n",
    "X_test = _c2[vis_features]\n",
    "y_test = _c2['label']\n",
    "true = y_test\n",
    "preds = model.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(model.predict(X_test))\n",
    "res.append(MeanBcubed(vb_truth, vb_pred))\n",
    "types.append('visual + d2v')\n",
    "trainings.append('c1c2')\n",
    "MeanBcubed(vb_truth, vb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train C2, Predict C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "size    34.403323\n",
       "P        0.471147\n",
       "R        0.905422\n",
       "F1       0.483325\n",
       "dtype: float64"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_features = ['font_diff3','crop_diff','header_diff','footer_diff', 'd2v_sim']\n",
    "model = LogisticRegression()\n",
    "model.fit(_c2[vis_features], _c2['label'])\n",
    "\n",
    "X_test = _c1[vis_features]\n",
    "y_test = _c1['label']\n",
    "true = y_test\n",
    "preds = model.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(model.predict(X_test))\n",
    "res.append(MeanBcubed(vb_truth, vb_pred))\n",
    "types.append('visual + d2v')\n",
    "trainings.append('c2c1')\n",
    "MeanBcubed(vb_truth, vb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_old = pd.read_csv('dfs/c1_old.csv',index_col = 0)\n",
    "c2_old = pd.read_csv('dfs/c2_old.csv',index_col = 0)\n",
    "c1_bert = pd.read_csv('sim_scores/bert_skip_c1.csv',index_col = 0)\n",
    "c2_bert = pd.read_csv('sim_scores/bert_skip_c2.csv',index_col = 0)\n",
    "\n",
    "v1 = pickle.load(open('pickles/c1_vectors.p','rb'))\n",
    "v2 = pickle.load(open('pickles/c2_vectors.p','rb'))\n",
    "\n",
    "v1_trunc = pickle.load(open('pickles/c1_vectors_trunc.p','rb'))\n",
    "v2_trunc = pickle.load(open('pickles/c2_vectors_trunc.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_old = c1_old.merge(c1_bert.drop(columns=['text_y_cleaned','label']), on = ['full_name','page'])\n",
    "c2_old = c2_old.merge(c2_bert.drop(columns=['text_y_cleaned','label']), on = ['full_name','page'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.39818276\n",
      "Iteration 2, loss = 0.35097355\n",
      "Iteration 3, loss = 0.33709768\n",
      "Iteration 4, loss = 0.32372174\n",
      "Iteration 5, loss = 0.31834743\n",
      "Iteration 6, loss = 0.30123198\n",
      "Iteration 7, loss = 0.28832314\n",
      "Iteration 8, loss = 0.28640865\n",
      "Iteration 9, loss = 0.27549390\n",
      "Iteration 10, loss = 0.26946782\n",
      "Iteration 11, loss = 0.25660425\n",
      "Iteration 12, loss = 0.25597089\n",
      "Iteration 13, loss = 0.25027705\n",
      "Iteration 14, loss = 0.26675862\n",
      "Iteration 15, loss = 0.23785877\n",
      "Iteration 16, loss = 0.23361837\n",
      "Iteration 17, loss = 0.22899692\n",
      "Iteration 18, loss = 0.22355790\n",
      "Iteration 19, loss = 0.23867769\n",
      "Iteration 20, loss = 0.22129997\n",
      "Iteration 21, loss = 0.22209691\n",
      "Iteration 22, loss = 0.21568291\n",
      "Iteration 23, loss = 0.20738741\n",
      "Iteration 24, loss = 0.20109768\n",
      "Iteration 25, loss = 0.19541929\n",
      "Iteration 26, loss = 0.21171757\n",
      "Iteration 27, loss = 0.21251098\n",
      "Iteration 28, loss = 0.18878991\n",
      "Iteration 29, loss = 0.18416842\n",
      "Iteration 30, loss = 0.17919205\n",
      "Iteration 31, loss = 0.17502174\n",
      "Iteration 32, loss = 0.17308155\n",
      "Iteration 33, loss = 0.17433345\n",
      "Iteration 34, loss = 0.16875687\n",
      "Iteration 35, loss = 0.16341826\n",
      "Iteration 36, loss = 0.16338385\n",
      "Iteration 37, loss = 0.15834445\n",
      "Iteration 38, loss = 0.15319855\n",
      "Iteration 39, loss = 0.15250924\n",
      "Iteration 40, loss = 0.14817995\n",
      "Iteration 41, loss = 0.14583128\n",
      "Iteration 42, loss = 0.14126797\n",
      "Iteration 43, loss = 0.14055650\n",
      "Iteration 44, loss = 0.14366512\n",
      "Iteration 45, loss = 0.13866658\n",
      "Iteration 46, loss = 0.13187732\n",
      "Iteration 47, loss = 0.13328957\n",
      "Iteration 48, loss = 0.12680241\n",
      "Iteration 49, loss = 0.12438587\n",
      "Iteration 50, loss = 0.12350234\n",
      "Iteration 51, loss = 0.11752158\n",
      "Iteration 52, loss = 0.11712138\n",
      "Iteration 53, loss = 0.11557942\n",
      "Iteration 54, loss = 0.11557177\n",
      "Iteration 55, loss = 0.11444979\n",
      "Iteration 56, loss = 0.11007443\n",
      "Iteration 57, loss = 0.11501961\n",
      "Iteration 58, loss = 0.10931413\n",
      "Iteration 59, loss = 0.10342131\n",
      "Iteration 60, loss = 0.10116885\n",
      "Iteration 61, loss = 0.11956614\n",
      "Iteration 62, loss = 0.09862769\n",
      "Iteration 63, loss = 0.09667178\n",
      "Iteration 64, loss = 0.09770681\n",
      "Iteration 65, loss = 0.09288025\n",
      "Iteration 66, loss = 0.09424464\n",
      "Iteration 67, loss = 0.09195991\n",
      "Iteration 68, loss = 0.09708802\n",
      "Iteration 69, loss = 0.08549444\n",
      "Iteration 70, loss = 0.08786494\n",
      "Iteration 71, loss = 0.08684835\n",
      "Iteration 72, loss = 0.08555336\n",
      "Iteration 73, loss = 0.08480582\n",
      "Iteration 74, loss = 0.07796854\n",
      "Iteration 75, loss = 0.07992916\n",
      "Iteration 76, loss = 0.08460144\n",
      "Iteration 77, loss = 0.08072651\n",
      "Iteration 78, loss = 0.07736309\n",
      "Iteration 79, loss = 0.07869893\n",
      "Iteration 80, loss = 0.07728902\n",
      "Iteration 81, loss = 0.07734811\n",
      "Iteration 82, loss = 0.07212756\n",
      "Iteration 83, loss = 0.07853796\n",
      "Iteration 84, loss = 0.07351568\n",
      "Iteration 85, loss = 0.08122032\n",
      "Iteration 86, loss = 0.07351811\n",
      "Iteration 87, loss = 0.06655323\n",
      "Iteration 88, loss = 0.06210863\n",
      "Iteration 89, loss = 0.07164568\n",
      "Iteration 90, loss = 0.06630690\n",
      "Iteration 91, loss = 0.06900904\n",
      "Iteration 92, loss = 0.06709586\n",
      "Iteration 93, loss = 0.06069909\n",
      "Iteration 94, loss = 0.05991393\n",
      "Iteration 95, loss = 0.07240597\n",
      "Iteration 96, loss = 0.06054610\n",
      "Iteration 97, loss = 0.06449951\n",
      "Iteration 98, loss = 0.05570555\n",
      "Iteration 99, loss = 0.06398595\n",
      "Iteration 100, loss = 0.05834163\n",
      "Iteration 101, loss = 0.05957387\n",
      "Iteration 102, loss = 0.05449837\n",
      "Iteration 103, loss = 0.05840268\n",
      "Iteration 104, loss = 0.05760027\n",
      "Iteration 105, loss = 0.05959214\n",
      "Iteration 106, loss = 0.06331756\n",
      "Iteration 107, loss = 0.05506540\n",
      "Iteration 108, loss = 0.05777912\n",
      "Iteration 109, loss = 0.05612122\n",
      "Iteration 110, loss = 0.05560045\n",
      "Iteration 111, loss = 0.04818635\n",
      "Iteration 112, loss = 0.05560123\n",
      "Iteration 113, loss = 0.05459408\n",
      "Iteration 114, loss = 0.04550065\n",
      "Iteration 115, loss = 0.04679356\n",
      "Iteration 116, loss = 0.05510775\n",
      "Iteration 117, loss = 0.04898075\n",
      "Iteration 118, loss = 0.05219200\n",
      "Iteration 119, loss = 0.05360159\n",
      "Iteration 120, loss = 0.04713586\n",
      "Iteration 121, loss = 0.05508084\n",
      "Iteration 122, loss = 0.04903695\n",
      "Iteration 123, loss = 0.05130526\n",
      "Iteration 124, loss = 0.05672261\n",
      "Iteration 125, loss = 0.04978115\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(batch_size=50, hidden_layer_sizes=(128,), verbose=1)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf1 = MLPClassifier(\n",
    "    hidden_layer_sizes = (128,),\n",
    "    batch_size = 50,\n",
    "    solver = 'adam',\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "clf1.fit(v1, c1_old['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.28963045\n",
      "Iteration 2, loss = 0.23539884\n",
      "Iteration 3, loss = 0.21925378\n",
      "Iteration 4, loss = 0.20706952\n",
      "Iteration 5, loss = 0.19985772\n",
      "Iteration 6, loss = 0.18956082\n",
      "Iteration 7, loss = 0.18630909\n",
      "Iteration 8, loss = 0.17956198\n",
      "Iteration 9, loss = 0.17534934\n",
      "Iteration 10, loss = 0.16988212\n",
      "Iteration 11, loss = 0.16726029\n",
      "Iteration 12, loss = 0.16236013\n",
      "Iteration 13, loss = 0.15889342\n",
      "Iteration 14, loss = 0.15642832\n",
      "Iteration 15, loss = 0.15356882\n",
      "Iteration 16, loss = 0.14868698\n",
      "Iteration 17, loss = 0.14696297\n",
      "Iteration 18, loss = 0.14379808\n",
      "Iteration 19, loss = 0.14002244\n",
      "Iteration 20, loss = 0.13480208\n",
      "Iteration 21, loss = 0.13391103\n",
      "Iteration 22, loss = 0.13007846\n",
      "Iteration 23, loss = 0.12879745\n",
      "Iteration 24, loss = 0.12386284\n",
      "Iteration 25, loss = 0.11920692\n",
      "Iteration 26, loss = 0.11859908\n",
      "Iteration 27, loss = 0.11438356\n",
      "Iteration 28, loss = 0.11697673\n",
      "Iteration 29, loss = 0.11021783\n",
      "Iteration 30, loss = 0.10827913\n",
      "Iteration 31, loss = 0.10750992\n",
      "Iteration 32, loss = 0.10504142\n",
      "Iteration 33, loss = 0.10229294\n",
      "Iteration 34, loss = 0.10533725\n",
      "Iteration 35, loss = 0.09840745\n",
      "Iteration 36, loss = 0.09369143\n",
      "Iteration 37, loss = 0.09139595\n",
      "Iteration 38, loss = 0.09377436\n",
      "Iteration 39, loss = 0.08748865\n",
      "Iteration 40, loss = 0.09103186\n",
      "Iteration 41, loss = 0.08467156\n",
      "Iteration 42, loss = 0.08089153\n",
      "Iteration 43, loss = 0.08454189\n",
      "Iteration 44, loss = 0.08283940\n",
      "Iteration 45, loss = 0.07894120\n",
      "Iteration 46, loss = 0.07839911\n",
      "Iteration 47, loss = 0.07360153\n",
      "Iteration 48, loss = 0.07265838\n",
      "Iteration 49, loss = 0.06986223\n",
      "Iteration 50, loss = 0.06785753\n",
      "Iteration 51, loss = 0.07066306\n",
      "Iteration 52, loss = 0.07366623\n",
      "Iteration 53, loss = 0.06682546\n",
      "Iteration 54, loss = 0.06452297\n",
      "Iteration 55, loss = 0.06321670\n",
      "Iteration 56, loss = 0.06556194\n",
      "Iteration 57, loss = 0.06800732\n",
      "Iteration 58, loss = 0.05841192\n",
      "Iteration 59, loss = 0.05664692\n",
      "Iteration 60, loss = 0.05745222\n",
      "Iteration 61, loss = 0.05681356\n",
      "Iteration 62, loss = 0.05464672\n",
      "Iteration 63, loss = 0.06024368\n",
      "Iteration 64, loss = 0.05860182\n",
      "Iteration 65, loss = 0.05208518\n",
      "Iteration 66, loss = 0.05650841\n",
      "Iteration 67, loss = 0.05093622\n",
      "Iteration 68, loss = 0.04822255\n",
      "Iteration 69, loss = 0.04630959\n",
      "Iteration 70, loss = 0.04753539\n",
      "Iteration 71, loss = 0.05503253\n",
      "Iteration 72, loss = 0.04818230\n",
      "Iteration 73, loss = 0.04750993\n",
      "Iteration 74, loss = 0.05043903\n",
      "Iteration 75, loss = 0.04201245\n",
      "Iteration 76, loss = 0.04227620\n",
      "Iteration 77, loss = 0.04378633\n",
      "Iteration 78, loss = 0.05007663\n",
      "Iteration 79, loss = 0.04432552\n",
      "Iteration 80, loss = 0.04260291\n",
      "Iteration 81, loss = 0.04598820\n",
      "Iteration 82, loss = 0.04231585\n",
      "Iteration 83, loss = 0.04494774\n",
      "Iteration 84, loss = 0.04054361\n",
      "Iteration 85, loss = 0.04307027\n",
      "Iteration 86, loss = 0.04305564\n",
      "Iteration 87, loss = 0.03600539\n",
      "Iteration 88, loss = 0.04563190\n",
      "Iteration 89, loss = 0.03749549\n",
      "Iteration 90, loss = 0.03557054\n",
      "Iteration 91, loss = 0.03650187\n",
      "Iteration 92, loss = 0.03824825\n",
      "Iteration 93, loss = 0.03808438\n",
      "Iteration 94, loss = 0.03918081\n",
      "Iteration 95, loss = 0.03810774\n",
      "Iteration 96, loss = 0.03663320\n",
      "Iteration 97, loss = 0.03746213\n",
      "Iteration 98, loss = 0.03102706\n",
      "Iteration 99, loss = 0.03726208\n",
      "Iteration 100, loss = 0.03416276\n",
      "Iteration 101, loss = 0.03760234\n",
      "Iteration 102, loss = 0.03238802\n",
      "Iteration 103, loss = 0.03316874\n",
      "Iteration 104, loss = 0.04593958\n",
      "Iteration 105, loss = 0.02959154\n",
      "Iteration 106, loss = 0.03444652\n",
      "Iteration 107, loss = 0.03413371\n",
      "Iteration 108, loss = 0.03702479\n",
      "Iteration 109, loss = 0.03271457\n",
      "Iteration 110, loss = 0.04075078\n",
      "Iteration 111, loss = 0.02975797\n",
      "Iteration 112, loss = 0.03402050\n",
      "Iteration 113, loss = 0.03093773\n",
      "Iteration 114, loss = 0.02838532\n",
      "Iteration 115, loss = 0.03550829\n",
      "Iteration 116, loss = 0.03462642\n",
      "Iteration 117, loss = 0.03148347\n",
      "Iteration 118, loss = 0.03001834\n",
      "Iteration 119, loss = 0.02946206\n",
      "Iteration 120, loss = 0.03675967\n",
      "Iteration 121, loss = 0.03377144\n",
      "Iteration 122, loss = 0.03089531\n",
      "Iteration 123, loss = 0.03492724\n",
      "Iteration 124, loss = 0.03167021\n",
      "Iteration 125, loss = 0.03954494\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(batch_size=50, hidden_layer_sizes=(128,), verbose=1)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = MLPClassifier(\n",
    "    hidden_layer_sizes = (128,),\n",
    "    batch_size = 50,\n",
    "    solver = 'adam',\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "clf2.fit(v2, c2_old['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2_old['bert_proba'] = clf1.predict_proba(v2)[:,1]\n",
    "c1_old['bert_proba'] = clf2.predict_proba(v1)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "_c1_old = c1_old[~pd.isnull(c1_old['text_y_cleaned'])]\n",
    "_c2_old = c2_old[~pd.isnull(c2_old['text_y_cleaned'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 200)\n",
    "# c1[['page','crop_diff','font_diff3','footer_diff','header_diff','label']][:200]\n",
    "# c1[c1['font_diff3'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on c1, predict c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Vectors only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "size    68.910383\n",
       "P        0.678032\n",
       "R        0.675426\n",
       "F1       0.523359\n",
       "dtype: float64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = v1\n",
    "y_train = c1_old['label']\n",
    "X_test = v2\n",
    "y_test = c2_old['label']\n",
    "\n",
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes = (128,),\n",
    "    batch_size = 50,\n",
    "    solver = 'adam',\n",
    "    verbose = 0\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "true = y_test\n",
    "preds = clf.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(clf.predict(X_test))\n",
    "res.append(MeanBcubed(vb_truth, vb_pred))\n",
    "types.append('bert only')\n",
    "trainings.append('c1c2')\n",
    "MeanBcubed(vb_truth, vb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "size    34.380975\n",
       "P        0.724989\n",
       "R        0.655715\n",
       "F1       0.545121\n",
       "dtype: float64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features = ['font_diff2','crop_diff','text_d2v_sim_score']\n",
    "X_train = v2\n",
    "y_train = c2_old['label']\n",
    "X_test = v1\n",
    "y_test = c1_old['label']\n",
    "\n",
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes = (128,),\n",
    "    batch_size = 50,\n",
    "    solver = 'adam',\n",
    "    verbose = 0\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "true = y_test\n",
    "preds = clf.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(clf.predict(X_test))\n",
    "res.append(MeanBcubed(vb_truth, vb_pred))\n",
    "types.append('bert only')\n",
    "trainings.append('c2c1')\n",
    "MeanBcubed(vb_truth, vb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual + Bert_similarity i-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "size    68.910383\n",
       "P        0.434505\n",
       "R        0.833364\n",
       "F1       0.355291\n",
       "dtype: float64"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['font_diff3','crop_diff','bert_sim']\n",
    "\n",
    "X_train = c1_old[features]\n",
    "y_train = c1_old['label']\n",
    "X_test = c2_old[features]\n",
    "y_test = c2_old['label']\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "true = y_test\n",
    "preds = model.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(model.predict(X_test))\n",
    "res.append(MeanBcubed(vb_truth, vb_pred))\n",
    "types.append('visual + bert similarity')\n",
    "trainings.append('c1c2')\n",
    "MeanBcubed(vb_truth, vb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "size    34.380975\n",
       "P        0.150656\n",
       "R        0.962881\n",
       "F1       0.183027\n",
       "dtype: float64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['font_diff3','crop_diff','bert_sim']\n",
    "\n",
    "X_train = c2_old[features]\n",
    "y_train = c2_old['label']\n",
    "X_test = c1_old[features]\n",
    "y_test = c1_old['label']\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "true = y_test\n",
    "preds = clf.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(clf.predict(X_test))\n",
    "res.append(MeanBcubed(vb_truth, vb_pred))\n",
    "types.append('visual + bert similarity')\n",
    "trainings.append('c2c1')\n",
    "MeanBcubed(vb_truth, vb_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual + Bert_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "size    68.910383\n",
       "P        0.548187\n",
       "R        0.769882\n",
       "F1       0.427405\n",
       "dtype: float64"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['font_diff3','crop_diff','bert_proba']\n",
    "\n",
    "X_train = c1_old[features]\n",
    "y_train = c1_old['label']\n",
    "X_test = c2_old[features]\n",
    "y_test = c2_old['label']\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "true = y_test\n",
    "preds = model.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(model.predict(X_test))\n",
    "res.append(MeanBcubed(vb_truth, vb_pred))\n",
    "types.append('visual + bert proba')\n",
    "trainings.append('c1c2')\n",
    "MeanBcubed(vb_truth, vb_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "size    34.380975\n",
       "P        0.304108\n",
       "R        0.969963\n",
       "F1       0.372951\n",
       "dtype: float64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['font_diff3','crop_diff','bert_proba']\n",
    "\n",
    "X_train = c2_old[features]\n",
    "y_train = c2_old['label']\n",
    "X_test = c1_old[features]\n",
    "y_test = c1_old['label']\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "true = y_test\n",
    "preds = clf.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(clf.predict(X_test))\n",
    "res.append(MeanBcubed(vb_truth, vb_pred))\n",
    "types.append('visual + bert proba')\n",
    "trainings.append('c2c1')\n",
    "MeanBcubed(vb_truth, vb_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP - BERT + Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19101/19101 [00:03<00:00, 4997.64it/s]\n",
      "100%|██████████| 16537/16537 [00:03<00:00, 5011.22it/s]\n"
     ]
    }
   ],
   "source": [
    "v1_plus = [np.append(v1[i], [c1_old.iloc[i]['font_diff3'], c1_old.iloc[i]['crop_diff']]) for i in tqdm(range(len(v1)))]\n",
    "v2_plus = [np.append(v2[i], [c2_old.iloc[i]['font_diff3'], c2_old.iloc[i]['crop_diff']]) for i in tqdm(range(len(v2)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train C1, Predict C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "size    68.910383\n",
       "P        0.657442\n",
       "R        0.704239\n",
       "F1       0.523575\n",
       "dtype: float64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features = ['font_diff2','crop_diff','text_d2v_sim_score']\n",
    "X_train = v1_plus\n",
    "y_train = c1_old['label']\n",
    "X_test = v2_plus\n",
    "y_test = c2_old['label']\n",
    "\n",
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes = (128,),\n",
    "    batch_size = 50,\n",
    "    solver = 'adam',\n",
    "    verbose = 0\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "true = y_test\n",
    "preds = clf.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(clf.predict(X_test))\n",
    "res.append(MeanBcubed(vb_truth, vb_pred))\n",
    "types.append('bert appended + MLP')\n",
    "trainings.append('c1c2')\n",
    "MeanBcubed(vb_truth, vb_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train C2, Predict C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "size    34.380975\n",
       "P        0.783772\n",
       "R        0.681577\n",
       "F1       0.601636\n",
       "dtype: float64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features = ['font_diff2','crop_diff','text_d2v_sim_score']\n",
    "X_train = v2_plus\n",
    "y_train = c2_old['label']\n",
    "X_test = v1_plus\n",
    "y_test = c1_old['label']\n",
    "\n",
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes = (128,),\n",
    "    batch_size = 50,\n",
    "    solver = 'adam',\n",
    "    verbose = 0\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "true = y_test\n",
    "preds = clf.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(clf.predict(X_test))\n",
    "res.append(MeanBcubed(vb_truth, vb_pred))\n",
    "types.append('bert appended + MLP')\n",
    "trainings.append('c2c1')\n",
    "MeanBcubed(vb_truth, vb_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">visual only</th>\n",
       "      <th>c1c2</th>\n",
       "      <td>69.707824</td>\n",
       "      <td>0.448520</td>\n",
       "      <td>0.827935</td>\n",
       "      <td>0.369598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c2c1</th>\n",
       "      <td>34.403323</td>\n",
       "      <td>0.369691</td>\n",
       "      <td>0.942905</td>\n",
       "      <td>0.404325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">visual + tfidf</th>\n",
       "      <th>c1c2</th>\n",
       "      <td>69.707824</td>\n",
       "      <td>0.462274</td>\n",
       "      <td>0.819096</td>\n",
       "      <td>0.379936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c2c1</th>\n",
       "      <td>34.403323</td>\n",
       "      <td>0.455809</td>\n",
       "      <td>0.942125</td>\n",
       "      <td>0.490227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">visual + d2v</th>\n",
       "      <th>c1c2</th>\n",
       "      <td>69.707824</td>\n",
       "      <td>0.559806</td>\n",
       "      <td>0.768265</td>\n",
       "      <td>0.446680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c2c1</th>\n",
       "      <td>34.403323</td>\n",
       "      <td>0.471147</td>\n",
       "      <td>0.905422</td>\n",
       "      <td>0.483325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">bert only</th>\n",
       "      <th>c1c2</th>\n",
       "      <td>68.910383</td>\n",
       "      <td>0.678032</td>\n",
       "      <td>0.675426</td>\n",
       "      <td>0.523359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c2c1</th>\n",
       "      <td>34.380975</td>\n",
       "      <td>0.724989</td>\n",
       "      <td>0.655715</td>\n",
       "      <td>0.545121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">visual + bert similarity</th>\n",
       "      <th>c1c2</th>\n",
       "      <td>68.910383</td>\n",
       "      <td>0.434505</td>\n",
       "      <td>0.833364</td>\n",
       "      <td>0.355291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c2c1</th>\n",
       "      <td>34.380975</td>\n",
       "      <td>0.150656</td>\n",
       "      <td>0.962881</td>\n",
       "      <td>0.183027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Visual + Bert Proba</th>\n",
       "      <th>c1c2</th>\n",
       "      <td>68.910383</td>\n",
       "      <td>0.548187</td>\n",
       "      <td>0.769882</td>\n",
       "      <td>0.427405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c2c1</th>\n",
       "      <td>34.380975</td>\n",
       "      <td>0.304108</td>\n",
       "      <td>0.969963</td>\n",
       "      <td>0.372951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">bert appended + MLP</th>\n",
       "      <th>c1c2</th>\n",
       "      <td>68.910383</td>\n",
       "      <td>0.657442</td>\n",
       "      <td>0.704239</td>\n",
       "      <td>0.523575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c2c1</th>\n",
       "      <td>34.380975</td>\n",
       "      <td>0.783772</td>\n",
       "      <td>0.681577</td>\n",
       "      <td>0.601636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    size         P         R        F1\n",
       "visual only              c1c2  69.707824  0.448520  0.827935  0.369598\n",
       "                         c2c1  34.403323  0.369691  0.942905  0.404325\n",
       "visual + tfidf           c1c2  69.707824  0.462274  0.819096  0.379936\n",
       "                         c2c1  34.403323  0.455809  0.942125  0.490227\n",
       "visual + d2v             c1c2  69.707824  0.559806  0.768265  0.446680\n",
       "                         c2c1  34.403323  0.471147  0.905422  0.483325\n",
       "bert only                c1c2  68.910383  0.678032  0.675426  0.523359\n",
       "                         c2c1  34.380975  0.724989  0.655715  0.545121\n",
       "visual + bert similarity c1c2  68.910383  0.434505  0.833364  0.355291\n",
       "                         c2c1  34.380975  0.150656  0.962881  0.183027\n",
       "Visual + Bert Proba      c1c2  68.910383  0.548187  0.769882  0.427405\n",
       "                         c2c1  34.380975  0.304108  0.969963  0.372951\n",
       "bert appended + MLP      c1c2  68.910383  0.657442  0.704239  0.523575\n",
       "                         c2c1  34.380975  0.783772  0.681577  0.601636"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = pd.DataFrame(res)\n",
    "b.index = [types,trainings]\n",
    "b"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0ef08d8ed6cc4ea55e1d44bd248b5018e6b6290053252328e46dd8d04768404"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('thesis8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
