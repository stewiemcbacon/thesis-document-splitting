{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyNVnKUABzq5"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "4_VRqdOhDebp",
        "outputId": "cdfe7bd9-f264-40e7-9811-8bc9dde918f8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\stefa\\anaconda3\\envs\\thesis8\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\stefa\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertConfig, RobertaForMaskedLM, RobertaTokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import re\n",
        "from scipy.spatial.distance import cosine\n",
        "from tqdm import tqdm\n",
        "\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import pickle\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTXu6KzeDkVk",
        "outputId": "fe19c312-32e3-4904-8fa1-8de38db8f8a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RobertaForMaskedLM(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(40000, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): RobertaLMHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (decoder): Linear(in_features=768, out_features=40000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_path = \"pdelobelle/robbert-v2-dutch-base\"\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
        "config = BertConfig.from_pretrained(model_path, output_hidden_states=True)\n",
        "\n",
        "robbert = RobertaForMaskedLM.from_pretrained(model_path, config=config)\n",
        "robbert.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FWGLQgwVCneV"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.read_csv('dfs/c1.csv',index_col = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dEkW-BiijwPj"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('dfs/c2.csv',index_col = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqUtX-3qSTzm",
        "outputId": "01aec753-6e5f-48eb-cd71-24f96a3f25a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 3/16537 [00:02<3:57:55,  1.16it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1840 > 512). Running this sequence through the model will result in indexing errors\n",
            "  0%|          | 25/16537 [00:21<4:14:59,  1.08it/s]"
          ]
        }
      ],
      "source": [
        "vectors = []\n",
        "fails = []\n",
        "for i in tqdm(range(len(df)), leave= True):\n",
        "    if pd.isnull(df.iloc[i]['text_y_cleaned']):\n",
        "        vectors.append(np.zeros(768))\n",
        "\n",
        "    else:\n",
        "        text = re.sub('\\n', ' ', df.iloc[i]['text_y'])\n",
        "\n",
        "        try:\n",
        "            vectors.append(robbert(**tokenizer(text, return_tensors='pt', padding=True))[1][-1][:, 0].detach().numpy())\n",
        "\n",
        "        except Exception as e:\n",
        "\n",
        "\n",
        "            tokens = text.split()\n",
        "            chunks = []\n",
        "            texts = []\n",
        "\n",
        "            if len(tokens) >= 200:\n",
        "                n_tokens = len(tokens)\n",
        "                n_chunks = int(np.ceil(n_tokens / 200))\n",
        "                balance = n_tokens/n_chunks\n",
        "\n",
        "                for j in range(n_chunks):\n",
        "                    split = int(j * balance)\n",
        "                    next_split = int((j+1) * balance)\n",
        "                    chunks.append(tokens[split:next_split])\n",
        "\n",
        "            for chunk in chunks:\n",
        "                texts.append(' '.join([x for x in chunk]))\n",
        "\n",
        "            try:\n",
        "                vectors.append(np.mean(robbert(**tokenizer(texts, return_tensors='pt', padding=True))[1][-1][:, 0].detach().numpy(), axis = 0))\n",
        "\n",
        "            except:\n",
        "                fails.append(i)\n",
        "                vectors.append(np.zeros(768))\n",
        "\n",
        "vectors = [x.flatten() for x in vectors]\n",
        "pickle.dump(vectors, open('pickles/c2_vectors.p','wb'))\n",
        "pickle.dump(fails, open('pickles/c2_fails.p','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/19102 [00:00<1:53:19,  2.81it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9768\\1323083704.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mvectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrobbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\stefa\\anaconda3\\envs\\thesis8\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\stefa\\anaconda3\\envs\\thesis8\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1103\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1105\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1106\u001b[0m         )\n\u001b[0;32m   1107\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\stefa\\anaconda3\\envs\\thesis8\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\stefa\\anaconda3\\envs\\thesis8\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    855\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    856\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 857\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    858\u001b[0m         )\n\u001b[0;32m    859\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\stefa\\anaconda3\\envs\\thesis8\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\stefa\\anaconda3\\envs\\thesis8\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    528\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m                     \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m                 )\n\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\stefa\\anaconda3\\envs\\thesis8\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\stefa\\anaconda3\\envs\\thesis8\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m         layer_output = apply_chunking_to_forward(\n\u001b[1;32m--> 451\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m         )\n\u001b[0;32m    453\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\stefa\\anaconda3\\envs\\thesis8\\lib\\site-packages\\transformers\\pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\stefa\\anaconda3\\envs\\thesis8\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[0mlayer_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\stefa\\anaconda3\\envs\\thesis8\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\stefa\\anaconda3\\envs\\thesis8\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "vectors = []\n",
        "fails = []\n",
        "for i in tqdm(range(len(df)), leave= True):\n",
        "    if pd.isnull(df.iloc[i]['text_y_cleaned']):\n",
        "        vectors.append(np.zeros(768))\n",
        "\n",
        "    else:\n",
        "        text = re.sub('\\n', ' ', df.iloc[i]['text_y'])\n",
        "\n",
        "        try:\n",
        "            vectors.append(robbert(**tokenizer(text, return_tensors='pt', padding=True, truncation=True))[1][-1][:, 0].detach().numpy())\n",
        "\n",
        "        except Exception as e:\n",
        "\n",
        "            fails.append(i)\n",
        "            vectors.append(np.zeros(768))\n",
        "\n",
        "vectors = [x.flatten() for x in vectors]\n",
        "pickle.dump(vectors, open('pickles/c1_vectors_trunc_c.p','wb'))\n",
        "pickle.dump(fails, open('pickles/c1_fails_trunc_c.p','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "c1 = pd.read_csv('dfs/c1_old.csv',index_col = 0)\n",
        "c2 = pd.read_csv('dfs/c2_old.csv',index_col = 0)\n",
        "\n",
        "v1 = pickle.load(open('pickles/c1_vectors.p','rb'))\n",
        "v2 = pickle.load(open('pickles/c2_vectors.p','rb'))\n",
        "\n",
        "v1_trunc = pickle.load(open('pickles/c1_vectors_trunc.p','rb'))\n",
        "v2_trunc = pickle.load(open('pickles/c2_vectors_trunc.p','rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare(df, v):\n",
        "    it = iter(range(1,len(df)))\n",
        "    sims = [0]\n",
        "\n",
        "    for i in tqdm(it):\n",
        "        a = v[i]\n",
        "        b = v[i-1]\n",
        "\n",
        "        if pd.isnull(df.iloc[i]['text_y_cleaned']):\n",
        "            j = i\n",
        "            skips = 1\n",
        "            j+=1\n",
        "            sims.append(0)\n",
        "\n",
        "            while pd.isnull(df.iloc[j]['text_y_cleaned']):\n",
        "                sims.append(0)\n",
        "                j+=1\n",
        "                skips+=1\n",
        "            \n",
        "            a = v[j]\n",
        "\n",
        "            sims.append(1-(cosine(a,b)))\n",
        "\n",
        "            for _ in range(skips):\n",
        "                i = next(it)\n",
        "\n",
        "        else:\n",
        "            sims.append(1-(cosine(a,b)))\n",
        "\n",
        "    return sims"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]c:\\Users\\stefa\\anaconda3\\envs\\thesis8\\lib\\site-packages\\scipy\\spatial\\distance.py:699: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
            "18116it [00:03, 5810.14it/s]\n"
          ]
        }
      ],
      "source": [
        "pd.set_option('display.max_rows', 1000)\n",
        "c1['bert_sim'] = compare(c1, v1)\n",
        "c1[['full_name','page','text_y_cleaned','bert_sim','label']].to_csv('sim_scores/bert_skip_c1.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "16294it [00:02, 6026.47it/s]\n"
          ]
        }
      ],
      "source": [
        "pd.set_option('display.max_rows', 1000)\n",
        "c2['bert_sim'] = compare(c2, v2)\n",
        "c2[['full_name','page','text_y_cleaned','bert_sim','label']].to_csv('sim_scores/bert_skip_c2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Twis02iGR4au",
        "outputId": "d89169ac-6cdd-4415-998d-9158a690c303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.28491390\n",
            "Iteration 2, loss = 0.23433349\n",
            "Iteration 3, loss = 0.21818552\n",
            "Iteration 4, loss = 0.20659018\n",
            "Iteration 5, loss = 0.19953856\n",
            "Iteration 6, loss = 0.18692595\n",
            "Iteration 7, loss = 0.18783377\n",
            "Iteration 8, loss = 0.18418299\n",
            "Iteration 9, loss = 0.17129857\n",
            "Iteration 10, loss = 0.16826632\n",
            "Iteration 11, loss = 0.16541426\n",
            "Iteration 12, loss = 0.16504572\n",
            "Iteration 13, loss = 0.15698980\n",
            "Iteration 14, loss = 0.15248672\n",
            "Iteration 15, loss = 0.14940498\n",
            "Iteration 16, loss = 0.15049046\n",
            "Iteration 17, loss = 0.14886559\n",
            "Iteration 18, loss = 0.14169094\n",
            "Iteration 19, loss = 0.13794728\n",
            "Iteration 20, loss = 0.13344064\n",
            "Iteration 21, loss = 0.13111730\n",
            "Iteration 22, loss = 0.12697437\n",
            "Iteration 23, loss = 0.12819952\n",
            "Iteration 24, loss = 0.12372083\n",
            "Iteration 25, loss = 0.12201273\n",
            "Iteration 26, loss = 0.11631380\n",
            "Iteration 27, loss = 0.11440534\n",
            "Iteration 28, loss = 0.11106603\n",
            "Iteration 29, loss = 0.11064234\n",
            "Iteration 30, loss = 0.11032870\n",
            "Iteration 31, loss = 0.10865633\n",
            "Iteration 32, loss = 0.10375366\n",
            "Iteration 33, loss = 0.10111000\n",
            "Iteration 34, loss = 0.09647101\n",
            "Iteration 35, loss = 0.09562587\n",
            "Iteration 36, loss = 0.09334130\n",
            "Iteration 37, loss = 0.08849771\n",
            "Iteration 38, loss = 0.09118281\n",
            "Iteration 39, loss = 0.08734484\n",
            "Iteration 40, loss = 0.08157333\n",
            "Iteration 41, loss = 0.08502223\n",
            "Iteration 42, loss = 0.08162979\n",
            "Iteration 43, loss = 0.07938613\n",
            "Iteration 44, loss = 0.07564368\n",
            "Iteration 45, loss = 0.07239148\n",
            "Iteration 46, loss = 0.07372079\n",
            "Iteration 47, loss = 0.07085081\n",
            "Iteration 48, loss = 0.07139071\n",
            "Iteration 49, loss = 0.07024907\n",
            "Iteration 50, loss = 0.06979242\n",
            "Iteration 51, loss = 0.06434357\n",
            "Iteration 52, loss = 0.06270838\n",
            "Iteration 53, loss = 0.06361896\n",
            "Iteration 54, loss = 0.05980460\n",
            "Iteration 55, loss = 0.06452046\n",
            "Iteration 56, loss = 0.05800061\n",
            "Iteration 57, loss = 0.05654272\n",
            "Iteration 58, loss = 0.05870057\n",
            "Iteration 59, loss = 0.05510562\n",
            "Iteration 60, loss = 0.05079772\n",
            "Iteration 61, loss = 0.05314509\n",
            "Iteration 62, loss = 0.05739372\n",
            "Iteration 63, loss = 0.05376288\n",
            "Iteration 64, loss = 0.04994638\n",
            "Iteration 65, loss = 0.05133335\n",
            "Iteration 66, loss = 0.04739998\n",
            "Iteration 67, loss = 0.05095262\n",
            "Iteration 68, loss = 0.04599929\n",
            "Iteration 69, loss = 0.04711036\n",
            "Iteration 70, loss = 0.05002545\n",
            "Iteration 71, loss = 0.04973606\n",
            "Iteration 72, loss = 0.04691642\n",
            "Iteration 73, loss = 0.04485906\n",
            "Iteration 74, loss = 0.04122266\n",
            "Iteration 75, loss = 0.04425103\n",
            "Iteration 76, loss = 0.04634063\n",
            "Iteration 77, loss = 0.04357114\n",
            "Iteration 78, loss = 0.04155154\n",
            "Iteration 79, loss = 0.04558664\n",
            "Iteration 80, loss = 0.04169613\n",
            "Iteration 81, loss = 0.03877251\n",
            "Iteration 82, loss = 0.03913851\n",
            "Iteration 83, loss = 0.04050272\n",
            "Iteration 84, loss = 0.04294355\n",
            "Iteration 85, loss = 0.03599432\n",
            "Iteration 86, loss = 0.04023003\n",
            "Iteration 87, loss = 0.04169733\n",
            "Iteration 88, loss = 0.03536956\n",
            "Iteration 89, loss = 0.03392643\n",
            "Iteration 90, loss = 0.03834603\n",
            "Iteration 91, loss = 0.03392553\n",
            "Iteration 92, loss = 0.03541012\n",
            "Iteration 93, loss = 0.03378106\n",
            "Iteration 94, loss = 0.03716401\n",
            "Iteration 95, loss = 0.04110352\n",
            "Iteration 96, loss = 0.03203126\n",
            "Iteration 97, loss = 0.03465550\n",
            "Iteration 98, loss = 0.03568721\n",
            "Iteration 99, loss = 0.03403564\n",
            "Iteration 100, loss = 0.03487442\n",
            "Iteration 101, loss = 0.03305354\n",
            "Iteration 102, loss = 0.04029468\n",
            "Iteration 103, loss = 0.04225281\n",
            "Iteration 104, loss = 0.03174324\n",
            "Iteration 105, loss = 0.03315673\n",
            "Iteration 106, loss = 0.03327314\n",
            "Iteration 107, loss = 0.03222251\n",
            "Iteration 108, loss = 0.04299745\n",
            "Iteration 109, loss = 0.02966261\n",
            "Iteration 110, loss = 0.02885575\n",
            "Iteration 111, loss = 0.03632097\n",
            "Iteration 112, loss = 0.03191413\n",
            "Iteration 113, loss = 0.02772637\n",
            "Iteration 114, loss = 0.03380655\n",
            "Iteration 115, loss = 0.03479288\n",
            "Iteration 116, loss = 0.03358301\n",
            "Iteration 117, loss = 0.02907064\n",
            "Iteration 118, loss = 0.02924596\n",
            "Iteration 119, loss = 0.03050953\n",
            "Iteration 120, loss = 0.03583638\n",
            "Iteration 121, loss = 0.03788824\n",
            "Iteration 122, loss = 0.02796517\n",
            "Iteration 123, loss = 0.03440610\n",
            "Iteration 124, loss = 0.03481416\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MLPClassifier(batch_size=50, hidden_layer_sizes=(128,), verbose=1)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "clf2 = MLPClassifier(\n",
        "    hidden_layer_sizes = (128,),\n",
        "    batch_size = 50,\n",
        "    solver = 'adam',\n",
        "    verbose = 1\n",
        ")\n",
        "\n",
        "clf2.fit(v2, c2['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.39970057\n",
            "Iteration 2, loss = 0.35566958\n",
            "Iteration 3, loss = 0.33829834\n",
            "Iteration 4, loss = 0.32052610\n",
            "Iteration 5, loss = 0.32782533\n",
            "Iteration 6, loss = 0.30855084\n",
            "Iteration 7, loss = 0.29661655\n",
            "Iteration 8, loss = 0.28572868\n",
            "Iteration 9, loss = 0.27781963\n",
            "Iteration 10, loss = 0.27187466\n",
            "Iteration 11, loss = 0.26379269\n",
            "Iteration 12, loss = 0.25832378\n",
            "Iteration 13, loss = 0.25375050\n",
            "Iteration 14, loss = 0.24374640\n",
            "Iteration 15, loss = 0.23886669\n",
            "Iteration 16, loss = 0.23181797\n",
            "Iteration 17, loss = 0.22543432\n",
            "Iteration 18, loss = 0.22296744\n",
            "Iteration 19, loss = 0.21625912\n",
            "Iteration 20, loss = 0.21498285\n",
            "Iteration 21, loss = 0.20395414\n",
            "Iteration 22, loss = 0.19965910\n",
            "Iteration 23, loss = 0.19340577\n",
            "Iteration 24, loss = 0.19073066\n",
            "Iteration 25, loss = 0.18470131\n",
            "Iteration 26, loss = 0.18144550\n",
            "Iteration 27, loss = 0.17510053\n",
            "Iteration 28, loss = 0.17735978\n",
            "Iteration 29, loss = 0.21596836\n",
            "Iteration 30, loss = 0.17847761\n",
            "Iteration 31, loss = 0.19982902\n",
            "Iteration 32, loss = 0.16684387\n",
            "Iteration 33, loss = 0.16370303\n",
            "Iteration 34, loss = 0.16295864\n",
            "Iteration 35, loss = 0.15757464\n",
            "Iteration 36, loss = 0.15323409\n",
            "Iteration 37, loss = 0.14933987\n",
            "Iteration 38, loss = 0.14313125\n",
            "Iteration 39, loss = 0.15159379\n",
            "Iteration 40, loss = 0.13864822\n",
            "Iteration 41, loss = 0.14475610\n",
            "Iteration 42, loss = 0.13053854\n",
            "Iteration 43, loss = 0.13230746\n",
            "Iteration 44, loss = 0.12688902\n",
            "Iteration 45, loss = 0.12457512\n",
            "Iteration 46, loss = 0.12662087\n",
            "Iteration 47, loss = 0.11530157\n",
            "Iteration 48, loss = 0.11788778\n",
            "Iteration 49, loss = 0.11753744\n",
            "Iteration 50, loss = 0.11489925\n",
            "Iteration 51, loss = 0.11723497\n",
            "Iteration 52, loss = 0.10842139\n",
            "Iteration 53, loss = 0.11920847\n",
            "Iteration 54, loss = 0.10647327\n",
            "Iteration 55, loss = 0.09897957\n",
            "Iteration 56, loss = 0.10243395\n",
            "Iteration 57, loss = 0.10309900\n",
            "Iteration 58, loss = 0.09872796\n",
            "Iteration 59, loss = 0.09704044\n",
            "Iteration 60, loss = 0.09352176\n",
            "Iteration 61, loss = 0.09268352\n",
            "Iteration 62, loss = 0.09023107\n",
            "Iteration 63, loss = 0.09171994\n",
            "Iteration 64, loss = 0.08610129\n",
            "Iteration 65, loss = 0.08796198\n",
            "Iteration 66, loss = 0.08239187\n",
            "Iteration 67, loss = 0.08454925\n",
            "Iteration 68, loss = 0.08269220\n",
            "Iteration 69, loss = 0.07723233\n",
            "Iteration 70, loss = 0.08139122\n",
            "Iteration 71, loss = 0.13290081\n",
            "Iteration 72, loss = 0.07966204\n",
            "Iteration 73, loss = 0.07677858\n",
            "Iteration 74, loss = 0.07746143\n",
            "Iteration 75, loss = 0.07192635\n",
            "Iteration 76, loss = 0.07764295\n",
            "Iteration 77, loss = 0.07406690\n",
            "Iteration 78, loss = 0.06481899\n",
            "Iteration 79, loss = 0.07603089\n",
            "Iteration 80, loss = 0.07033338\n",
            "Iteration 81, loss = 0.06890123\n",
            "Iteration 82, loss = 0.06753991\n",
            "Iteration 83, loss = 0.06976541\n",
            "Iteration 84, loss = 0.07590436\n",
            "Iteration 85, loss = 0.06122879\n",
            "Iteration 86, loss = 0.06082759\n",
            "Iteration 87, loss = 0.05940279\n",
            "Iteration 88, loss = 0.06400649\n",
            "Iteration 89, loss = 0.06769943\n",
            "Iteration 90, loss = 0.06013291\n",
            "Iteration 91, loss = 0.06028658\n",
            "Iteration 92, loss = 0.05812951\n",
            "Iteration 93, loss = 0.05864777\n",
            "Iteration 94, loss = 0.05746377\n",
            "Iteration 95, loss = 0.06424135\n",
            "Iteration 96, loss = 0.06062354\n",
            "Iteration 97, loss = 0.05758990\n",
            "Iteration 98, loss = 0.07586115\n",
            "Iteration 99, loss = 0.04835348\n",
            "Iteration 100, loss = 0.06279562\n",
            "Iteration 101, loss = 0.05001021\n",
            "Iteration 102, loss = 0.05462568\n",
            "Iteration 103, loss = 0.05368937\n",
            "Iteration 104, loss = 0.04862125\n",
            "Iteration 105, loss = 0.12787603\n",
            "Iteration 106, loss = 0.05153518\n",
            "Iteration 107, loss = 0.05028653\n",
            "Iteration 108, loss = 0.05083926\n",
            "Iteration 109, loss = 0.05000833\n",
            "Iteration 110, loss = 0.04986793\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MLPClassifier(batch_size=50, hidden_layer_sizes=(128,), verbose=1)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf1 = MLPClassifier(\n",
        "    hidden_layer_sizes = (128,),\n",
        "    batch_size = 50,\n",
        "    solver = 'adam',\n",
        "    verbose = 1\n",
        ")\n",
        "\n",
        "clf1.fit(v1, c1['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "c2['bert_proba'] = clf1.predict_proba(v2)[:,1]\n",
        "c1['bert_proba'] = clf2.predict_proba(v1)[:,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yto4MJj3UnQg",
        "outputId": "3601be66-c781-4daf-ff72-92e24627203b"
      },
      "outputs": [],
      "source": [
        "def change_format(y):\n",
        "    y[0] = 1\n",
        "    indices = [i for i, x in enumerate(y) if x == 1]+[len(y)-1]\n",
        "    result = []\n",
        "    for i in range(len(indices)):\n",
        "        if i != len(indices)-1:\n",
        "            result.append(indices[i+1] - indices[i])\n",
        "    result[-1]+=1\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_index(split):\n",
        "    '''Turns a doc length vector like [1,2,1,3,3,5] into a dict with pagenumbers as keys and the set of all \n",
        "    pagenumbers in the same document as value.\n",
        "    This thus is an index which gives for every page its cluster.'''\n",
        "    l= sum(split)\n",
        "    pages= list(np.arange(l))\n",
        "    out = defaultdict(set)\n",
        "    for block_length in split:\n",
        "        block= pages[:block_length]\n",
        "        pages= pages[block_length:]\n",
        "        for page in block:\n",
        "            out[page]= set(block)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Bcubed(truth,pred):\n",
        "    assert sum(truth)==sum(pred)  # same amount of pages\n",
        "    truth,pred = make_index(truth), make_index(pred)\n",
        "    \n",
        "    df  ={i:{'size':len(truth[i]),'P':0,'R':0,'F1':0} for i in truth}\n",
        "    for i in truth:\n",
        "        df[i]['P']= len(truth[i] & pred[i])/len(pred[i]) \n",
        "        df[i]['R']= len(truth[i] & pred[i])/len(truth[i])\n",
        "        df[i]['F1']= (2*df[i]['P']*df[i]['R'])/(df[i]['P']+df[i]['R'])\n",
        "    df= pd.DataFrame.from_dict(df, orient='index')\n",
        "    df.index_name='PageNr'\n",
        "    return  df\n",
        "\n",
        "\n",
        "def MeanBcubed(truth,pred):\n",
        "    assert sum(truth)==sum(pred)  # same amount of pages\n",
        "    return Bcubed(truth,pred).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lLHXwoqbEMlE",
        "outputId": "cafc8be1-5c6c-4abb-c727-53dae3faa13d"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from collections import defaultdict\n",
        "\n",
        "_c1 = c1[~pd.isnull(c1['text_y_cleaned'])]\n",
        "_c2 = c2[~pd.isnull(c2['text_y_cleaned'])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train on c1, predict c2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bert Vectors only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size    68.910383\n",
            "P        0.625141\n",
            "R        0.719308\n",
            "F1       0.513387\n",
            "dtype: float64\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.4579242636746143, 0.3075836081017428, 0.36799098337559877)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train = v1\n",
        "y_train = c1['label']\n",
        "X_test = v2\n",
        "y_test = c2['label']\n",
        "\n",
        "clf = MLPClassifier(\n",
        "    hidden_layer_sizes = (128,),\n",
        "    batch_size = 50,\n",
        "    solver = 'adam',\n",
        "    verbose = 0\n",
        ")\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "true = y_test\n",
        "preds = clf.predict(X_test)\n",
        "vb_truth, vb_pred = change_format(y_test.values), change_format(clf.predict(X_test))\n",
        "print(MeanBcubed(vb_truth,vb_pred))\n",
        "precision_score(true, preds), recall_score(true, preds), f1_score(true, preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visual + Bert_similarity i-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size    68.910383\n",
            "P        0.460502\n",
            "R        0.820065\n",
            "F1       0.369308\n",
            "dtype: float64\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.4175084175084175, 0.2920395666509656, 0.3436807095343681)"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features = ['font_diff2','crop_diff','bert_sim']\n",
        "\n",
        "X_train = c1[features]\n",
        "y_train = c1['label']\n",
        "X_test = c2[features]\n",
        "y_test = c2['label']\n",
        "\n",
        "model = LogisticRegression()\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "true = y_test\n",
        "preds = model.predict(X_test)\n",
        "vb_truth, vb_pred = change_format(y_test.values), change_format(model.predict(X_test))\n",
        "print(MeanBcubed(vb_truth,vb_pred))\n",
        "precision_score(true, preds), recall_score(true, preds), f1_score(true, preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visual + Bert_proba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size    68.910383\n",
            "P        0.621317\n",
            "R        0.717870\n",
            "F1       0.447542\n",
            "dtype: float64\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.3566299664911441, 0.3509185115402732, 0.3537511870845204)"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features = ['font_diff2','crop_diff','bert_proba']\n",
        "\n",
        "X_train = c1[features]\n",
        "y_train = c1['label']\n",
        "X_test = c2[features]\n",
        "y_test = c2['label']\n",
        "\n",
        "model = LogisticRegression()\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "true = y_test\n",
        "preds = model.predict(X_test)\n",
        "vb_truth, vb_pred = change_format(y_test.values), change_format(model.predict(X_test))\n",
        "print(MeanBcubed(vb_truth,vb_pred))\n",
        "precision_score(true, preds), recall_score(true, preds), f1_score(true, preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train on c2, predict c1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bert vectors only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size    34.380975\n",
            "P        0.654473\n",
            "R        0.730418\n",
            "F1       0.550707\n",
            "dtype: float64\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.5359669811320755, 0.46448645886561063, 0.49767314535997814)"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# features = ['font_diff2','crop_diff','text_d2v_sim_score']\n",
        "X_train = v2\n",
        "y_train = c2['label']\n",
        "X_test = v1\n",
        "y_test = c1['label']\n",
        "\n",
        "clf = MLPClassifier(\n",
        "    hidden_layer_sizes = (128,),\n",
        "    batch_size = 50,\n",
        "    solver = 'adam',\n",
        "    verbose = 0\n",
        ")\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "true = y_test\n",
        "preds = clf.predict(X_test)\n",
        "vb_truth, vb_pred = change_format(y_test.values), change_format(clf.predict(X_test))\n",
        "print(MeanBcubed(vb_truth,vb_pred))\n",
        "precision_score(true, preds), recall_score(true, preds), f1_score(true, preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visual + Bert_similarity i-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size    34.380975\n",
            "P        0.220258\n",
            "R        0.931222\n",
            "F1       0.240076\n",
            "dtype: float64\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.36087484811664644, 0.07588145120081757, 0.12539582013932868)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features = ['font_diff2','crop_diff','bert_sim']\n",
        "\n",
        "X_train = c2[features]\n",
        "y_train = c2['label']\n",
        "X_test = c1[features]\n",
        "y_test = c1['label']\n",
        "\n",
        "clf = LogisticRegression()\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "true = y_test\n",
        "preds = clf.predict(X_test)\n",
        "vb_truth, vb_pred = change_format(y_test.values), change_format(clf.predict(X_test))\n",
        "print(MeanBcubed(vb_truth,vb_pred))\n",
        "precision_score(true, preds), recall_score(true, preds), f1_score(true, preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visual + Bert_proba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size    34.380975\n",
            "P        0.337096\n",
            "R        0.964473\n",
            "F1       0.408648\n",
            "dtype: float64\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.8282352941176471, 0.17986714358712313, 0.29554995801847184)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features = ['font_diff2','crop_diff','bert_proba']\n",
        "\n",
        "X_train = c2[features]\n",
        "y_train = c2['label']\n",
        "X_test = c1[features]\n",
        "y_test = c1['label']\n",
        "\n",
        "clf = LogisticRegression()\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "true = y_test\n",
        "preds = clf.predict(X_test)\n",
        "vb_truth, vb_pred = change_format(y_test.values), change_format(clf.predict(X_test))\n",
        "print(MeanBcubed(vb_truth,vb_pred))\n",
        "precision_score(true, preds), recall_score(true, preds), f1_score(true, preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 19101/19101 [00:03<00:00, 5088.17it/s]\n",
            "100%|| 16537/16537 [00:03<00:00, 5063.69it/s]\n"
          ]
        }
      ],
      "source": [
        "v1_plus = [np.append(v1[i], [c1.iloc[i]['font_diff2'], c1.iloc[i]['crop_diff']]) for i in tqdm(range(len(v1)))]\n",
        "v2_plus = [np.append(v2[i], [c1.iloc[i]['font_diff2'], c1.iloc[i]['crop_diff']]) for i in tqdm(range(len(v2)))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size    34.380975\n",
            "P        0.768560\n",
            "R        0.649223\n",
            "F1       0.568055\n",
            "dtype: float64\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.49170305676855897, 0.575370464997445, 0.5302566517541795)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# features = ['font_diff2','crop_diff','text_d2v_sim_score']\n",
        "X_train = v2_plus\n",
        "y_train = c2['label']\n",
        "X_test = v1_plus\n",
        "y_test = c1['label']\n",
        "\n",
        "clf = MLPClassifier(\n",
        "    hidden_layer_sizes = (128,),\n",
        "    batch_size = 50,\n",
        "    solver = 'adam',\n",
        "    verbose = 0\n",
        ")\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "true = y_test\n",
        "preds = clf.predict(X_test)\n",
        "vb_truth, vb_pred = change_format(y_test.values), change_format(clf.predict(X_test))\n",
        "print(MeanBcubed(vb_truth,vb_pred))\n",
        "precision_score(true, preds), recall_score(true, preds), f1_score(true, preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'v1_plus' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17792\\1358449777.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# features = ['font_diff2','crop_diff','text_d2v_sim_score']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv1_plus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv2_plus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'v1_plus' is not defined"
          ]
        }
      ],
      "source": [
        "# features = ['font_diff2','crop_diff','text_d2v_sim_score']\n",
        "X_train = v1_plus\n",
        "y_train = c1['label']\n",
        "X_test = v2_plus\n",
        "y_test = c2['label']\n",
        "\n",
        "clf = MLPClassifier(\n",
        "    hidden_layer_sizes = (128,),\n",
        "    batch_size = 50,\n",
        "    solver = 'adam',\n",
        "    verbose = 0\n",
        ")\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "true = y_test\n",
        "preds = clf.predict(X_test)\n",
        "vb_truth, vb_pred = change_format(y_test.values), change_format(clf.predict(X_test))\n",
        "print(MeanBcubed(vb_truth,vb_pred))\n",
        "precision_score(true, preds), recall_score(true, preds), f1_score(true, preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size    68.910383\n",
            "P        0.597625\n",
            "R        0.716711\n",
            "F1       0.495238\n",
            "dtype: float64\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.40730337078651685, 0.20489872821479038, 0.2726418050767784)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# features = ['font_diff2','crop_diff','text_d2v_sim_score']\n",
        "X_train = v1_trunc\n",
        "y_train = c1['label']\n",
        "X_test = v2_trunc\n",
        "y_test = c2['label']\n",
        "\n",
        "clf = MLPClassifier(\n",
        "    hidden_layer_sizes = (128,),\n",
        "    batch_size = 50,\n",
        "    solver = 'adam',\n",
        "    verbose = 0\n",
        ")\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "true = y_test\n",
        "preds = clf.predict(X_test)\n",
        "vb_truth, vb_pred = change_format(y_test.values), change_format(clf.predict(X_test))\n",
        "print(MeanBcubed(vb_truth,vb_pred))\n",
        "precision_score(true, preds), recall_score(true, preds), f1_score(true, preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size    34.380975\n",
            "P        0.798517\n",
            "R        0.616726\n",
            "F1       0.570875\n",
            "dtype: float64\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.48612497504491914, 0.6221257026060296, 0.5457805670738541)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# features = ['font_diff2','crop_diff','text_d2v_sim_score']\n",
        "X_train = v2_trunc\n",
        "y_train = c2['label']\n",
        "X_test = v1_trunc\n",
        "y_test = c1['label']\n",
        "\n",
        "clf = MLPClassifier(\n",
        "    hidden_layer_sizes = (128,),\n",
        "    batch_size = 50,\n",
        "    solver = 'adam',\n",
        "    verbose = 0\n",
        ")\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "true = y_test\n",
        "preds = clf.predict(X_test)\n",
        "vb_truth, vb_pred = change_format(y_test.values), change_format(clf.predict(X_test))\n",
        "print(MeanBcubed(vb_truth,vb_pred))\n",
        "precision_score(true, preds), recall_score(true, preds), f1_score(true, preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 19101/19101 [00:03<00:00, 4838.15it/s]\n",
            "100%|| 16537/16537 [00:03<00:00, 4907.12it/s]\n"
          ]
        }
      ],
      "source": [
        "v1_plus_trunc = [np.append(v1_trunc[i], [c1.iloc[i]['font_diff2'], c1.iloc[i]['crop_diff']]) for i in tqdm(range(len(v1_trunc)))]\n",
        "v2_plus_trunc = [np.append(v2_trunc[i], [c1.iloc[i]['font_diff2'], c1.iloc[i]['crop_diff']]) for i in tqdm(range(len(v2_trunc)))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size    68.910383\n",
            "P        0.622936\n",
            "R        0.637141\n",
            "F1       0.456531\n",
            "dtype: float64\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.3352984524686809, 0.2143193593970796, 0.2614942528735632)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# features = ['font_diff2','crop_diff','text_d2v_sim_score']\n",
        "X_train = v1_plus_trunc\n",
        "y_train = c1['label']\n",
        "X_test = v2_plus_trunc\n",
        "y_test = c2['label']\n",
        "\n",
        "clf = MLPClassifier(\n",
        "    hidden_layer_sizes = (128,),\n",
        "    batch_size = 50,\n",
        "    solver = 'adam',\n",
        "    verbose = 0\n",
        ")\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "true = y_test\n",
        "preds = clf.predict(X_test)\n",
        "vb_truth, vb_pred = change_format(y_test.values), change_format(clf.predict(X_test))\n",
        "print(MeanBcubed(vb_truth,vb_pred))\n",
        "precision_score(true, preds), recall_score(true, preds), f1_score(true, preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size    34.380975\n",
            "P        0.791557\n",
            "R        0.625336\n",
            "F1       0.566781\n",
            "dtype: float64\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.4886924342105263, 0.6073071027082269, 0.541581225791752)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# features = ['font_diff2','crop_diff','text_d2v_sim_score']\n",
        "X_train = v2_plus_trunc\n",
        "y_train = c2['label']\n",
        "X_test = v1_plus_trunc\n",
        "y_test = c1['label']\n",
        "\n",
        "clf = MLPClassifier(\n",
        "    hidden_layer_sizes = (128,),\n",
        "    batch_size = 50,\n",
        "    solver = 'adam',\n",
        "    verbose = 0\n",
        ")\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "true = y_test\n",
        "preds = clf.predict(X_test)\n",
        "vb_truth, vb_pred = change_format(y_test.values), change_format(clf.predict(X_test))\n",
        "print(MeanBcubed(vb_truth,vb_pred))\n",
        "precision_score(true, preds), recall_score(true, preds), f1_score(true, preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib as plt\n",
        "\n",
        "_c1['bert_proba'][:100].plot()\n",
        "_c1['label'].apply(lambda x: 0 if x == 1 else 1)[:100].plot()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "bert.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "b0ef08d8ed6cc4ea55e1d44bd248b5018e6b6290053252328e46dd8d04768404"
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('thesis8')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
