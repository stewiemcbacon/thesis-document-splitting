{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_hf = pd.read_csv('sim_scores/c1_headerfooter.csv', index_col = 0)\n",
    "c2_hf = pd.read_csv('sim_scores/c2_headerfooter.csv', index_col = 0)\n",
    "c1_d2v = pd.read_csv('sim_scores/d2v_skip_c1.csv', index_col = 0)\n",
    "c2_d2v = pd.read_csv('sim_scores/d2v_skip_c2.csv', index_col = 0)\n",
    "c1_tfidf = pd.read_csv('sim_scores/tfidf_skip_c1.csv', index_col = 0)\n",
    "c2_tfidf = pd.read_csv('sim_scores/tfidf_skip_c2.csv', index_col = 0)\n",
    "c1 = pd.read_csv('dfs/c1.csv', index_col = 0)\n",
    "c2 = pd.read_csv('dfs/c2.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = c1.merge(c1_hf.drop(columns=['label']), how='left', on=['file_name','page']).merge(c1_d2v.drop(columns=['text_y_cleaned','label']), how='left',on=['full_name','page']).merge(c1_tfidf.drop(columns=['text_y_cleaned','label']), how='left',on=['full_name','page'])\n",
    "c2 = c2.merge(c2_hf.drop(columns=['label']), how='left', on=['file_name','page']).merge(c2_d2v.drop(columns=['text_y_cleaned','label']), how='left',on=['full_name','page']).merge(c2_tfidf.drop(columns=['text_y_cleaned','label']), how='left',on=['full_name','page'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_format(y):\n",
    "    y[0] = 1\n",
    "    indices = [i for i, x in enumerate(y) if x == 1]+[len(y)-1]\n",
    "    result = []\n",
    "    for i in range(len(indices)):\n",
    "        if i != len(indices)-1:\n",
    "            result.append(indices[i+1] - indices[i])\n",
    "    result[-1]+=1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_index(split):\n",
    "    '''Turns a doc length vector like [1,2,1,3,3,5] into a dict with pagenumbers as keys and the set of all \n",
    "    pagenumbers in the same document as value.\n",
    "    This thus is an index which gives for every page its cluster.'''\n",
    "    l= sum(split)\n",
    "    pages= list(np.arange(l))\n",
    "    out = defaultdict(set)\n",
    "    for block_length in split:\n",
    "        block= pages[:block_length]\n",
    "        pages= pages[block_length:]\n",
    "        for page in block:\n",
    "            out[page]= set(block)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bcubed(truth,pred):\n",
    "    assert sum(truth)==sum(pred)  # same amount of pages\n",
    "    truth,pred = make_index(truth), make_index(pred)\n",
    "    \n",
    "    df  ={i:{'size':len(truth[i]),'P':0,'R':0,'F1':0} for i in truth}\n",
    "    for i in truth:\n",
    "        df[i]['P']= len(truth[i] & pred[i])/len(pred[i]) \n",
    "        df[i]['R']= len(truth[i] & pred[i])/len(truth[i])\n",
    "        df[i]['F1']= (2*df[i]['P']*df[i]['R'])/(df[i]['P']+df[i]['R'])\n",
    "    df= pd.DataFrame.from_dict(df, orient='index')\n",
    "    df.index_name='PageNr'\n",
    "    return  df\n",
    "\n",
    "\n",
    "def MeanBcubed(truth,pred):\n",
    "    assert sum(truth)==sum(pred)  # same amount of pages\n",
    "    return Bcubed(truth,pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from collections import defaultdict\n",
    "\n",
    "_c1 = c1[~pd.isnull(c1['text_y_cleaned'])]\n",
    "_c2 = c2[~pd.isnull(c2['text_y_cleaned'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual features Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train C1, Predict C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "size    69.707824\n",
       "P        0.448520\n",
       "R        0.827935\n",
       "F1       0.369598\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_features = ['font_diff3','crop_diff','header_diff','footer_diff']\n",
    "model = LogisticRegression()\n",
    "model.fit(_c1[vis_features], _c1['label'])\n",
    "\n",
    "X_test = _c2[vis_features]\n",
    "y_test = _c2['label']\n",
    "true = y_test\n",
    "preds = model.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(model.predict(X_test))\n",
    "MeanBcubed(vb_truth, vb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train C2, Predict C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "size    34.403323\n",
       "P        0.369691\n",
       "R        0.942905\n",
       "F1       0.404325\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_features = ['font_diff3','crop_diff','header_diff','footer_diff']\n",
    "model = LogisticRegression()\n",
    "model.fit(_c2[vis_features], _c2['label'])\n",
    "\n",
    "X_test = _c1[vis_features]\n",
    "y_test = _c1['label']\n",
    "true = y_test\n",
    "preds = model.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(model.predict(X_test))\n",
    "MeanBcubed(vb_truth, vb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF + visual results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train C1, predict C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "size    69.707824\n",
       "P        0.462274\n",
       "R        0.819096\n",
       "F1       0.379936\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_features = ['font_diff3','crop_diff','header_diff','footer_diff','tfidf_sim']\n",
    "model = LogisticRegression()\n",
    "model.fit(_c1[vis_features], _c1['label'])\n",
    "\n",
    "X_test = _c2[vis_features]\n",
    "y_test = _c2['label']\n",
    "true = y_test\n",
    "preds = model.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(model.predict(X_test))\n",
    "MeanBcubed(vb_truth, vb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train C2, Predict C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "size    34.403323\n",
       "P        0.455809\n",
       "R        0.942125\n",
       "F1       0.490227\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_features = ['font_diff3','crop_diff','header_diff','footer_diff', 'tfidf_sim']\n",
    "model = LogisticRegression()\n",
    "model.fit(_c2[vis_features], _c2['label'])\n",
    "\n",
    "X_test = _c1[vis_features]\n",
    "y_test = _c1['label']\n",
    "true = y_test\n",
    "preds = model.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(model.predict(X_test))\n",
    "MeanBcubed(vb_truth, vb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D2V + Visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train C1, predict C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "size    69.707824\n",
       "P        0.559806\n",
       "R        0.768265\n",
       "F1       0.446680\n",
       "dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_features = ['font_diff3','crop_diff','header_diff','footer_diff','d2v_sim']\n",
    "model = LogisticRegression()\n",
    "model.fit(_c1[vis_features], _c1['label'])\n",
    "\n",
    "X_test = _c2[vis_features]\n",
    "y_test = _c2['label']\n",
    "true = y_test\n",
    "preds = model.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(model.predict(X_test))\n",
    "MeanBcubed(vb_truth, vb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train C2, Predict C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "size    34.403323\n",
       "P        0.471147\n",
       "R        0.905422\n",
       "F1       0.483325\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis_features = ['font_diff3','crop_diff','header_diff','footer_diff', 'd2v_sim']\n",
    "model = LogisticRegression()\n",
    "model.fit(_c2[vis_features], _c2['label'])\n",
    "\n",
    "X_test = _c1[vis_features]\n",
    "y_test = _c1['label']\n",
    "true = y_test\n",
    "preds = model.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(model.predict(X_test))\n",
    "MeanBcubed(vb_truth, vb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_old = pd.read_csv('dfs/c1_old.csv',index_col = 0)\n",
    "c2_old = pd.read_csv('dfs/c2_old.csv',index_col = 0)\n",
    "c1_bert = pd.read_csv('sim_scores/bert_skip_c1.csv',index_col = 0)\n",
    "c2_bert = pd.read_csv('sim_scores/bert_skip_c2.csv',index_col = 0)\n",
    "\n",
    "v1 = pickle.load(open('pickles/c1_vectors.p','rb'))\n",
    "v2 = pickle.load(open('pickles/c2_vectors.p','rb'))\n",
    "\n",
    "v1_trunc = pickle.load(open('pickles/c1_vectors_trunc.p','rb'))\n",
    "v2_trunc = pickle.load(open('pickles/c2_vectors_trunc.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_old = c1_old.merge(c1_bert.drop(columns=['text_y_cleaned','label']), on = ['full_name','page'])\n",
    "c2_old = c2_old.merge(c2_bert.drop(columns=['text_y_cleaned','label']), on = ['full_name','page'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.40946184\n",
      "Iteration 2, loss = 0.36183279\n",
      "Iteration 3, loss = 0.34101073\n",
      "Iteration 4, loss = 0.33127445\n",
      "Iteration 5, loss = 0.31804524\n",
      "Iteration 6, loss = 0.30892639\n",
      "Iteration 7, loss = 0.30077472\n",
      "Iteration 8, loss = 0.30326847\n",
      "Iteration 9, loss = 0.29066891\n",
      "Iteration 10, loss = 0.27566614\n",
      "Iteration 11, loss = 0.27167211\n",
      "Iteration 12, loss = 0.28626322\n",
      "Iteration 13, loss = 0.26632670\n",
      "Iteration 14, loss = 0.26244806\n",
      "Iteration 15, loss = 0.25046571\n",
      "Iteration 16, loss = 0.24937064\n",
      "Iteration 17, loss = 0.24486053\n",
      "Iteration 18, loss = 0.23642340\n",
      "Iteration 19, loss = 0.23615901\n",
      "Iteration 20, loss = 0.22873189\n",
      "Iteration 21, loss = 0.22257550\n",
      "Iteration 22, loss = 0.21612877\n",
      "Iteration 23, loss = 0.21477054\n",
      "Iteration 24, loss = 0.21301685\n",
      "Iteration 25, loss = 0.20159179\n",
      "Iteration 26, loss = 0.21875542\n",
      "Iteration 27, loss = 0.20296074\n",
      "Iteration 28, loss = 0.19578381\n",
      "Iteration 29, loss = 0.21499388\n",
      "Iteration 30, loss = 0.22900750\n",
      "Iteration 31, loss = 0.19626225\n",
      "Iteration 32, loss = 0.19317204\n",
      "Iteration 33, loss = 0.19025817\n",
      "Iteration 34, loss = 0.18285110\n",
      "Iteration 35, loss = 0.18016004\n",
      "Iteration 36, loss = 0.20409672\n",
      "Iteration 37, loss = 0.18078510\n",
      "Iteration 38, loss = 0.17611095\n",
      "Iteration 39, loss = 0.17689430\n",
      "Iteration 40, loss = 0.17015338\n",
      "Iteration 41, loss = 0.17137066\n",
      "Iteration 42, loss = 0.16561638\n",
      "Iteration 43, loss = 0.16181212\n",
      "Iteration 44, loss = 0.18858373\n",
      "Iteration 45, loss = 0.16120691\n",
      "Iteration 46, loss = 0.16439045\n",
      "Iteration 47, loss = 0.15680194\n",
      "Iteration 48, loss = 0.15529467\n",
      "Iteration 49, loss = 0.15085722\n",
      "Iteration 50, loss = 0.19111472\n",
      "Iteration 51, loss = 0.16077972\n",
      "Iteration 52, loss = 0.15874830\n",
      "Iteration 53, loss = 0.15717277\n",
      "Iteration 54, loss = 0.15497606\n",
      "Iteration 55, loss = 0.14893473\n",
      "Iteration 56, loss = 0.14773156\n",
      "Iteration 57, loss = 0.14757571\n",
      "Iteration 58, loss = 0.14044392\n",
      "Iteration 59, loss = 0.13841273\n",
      "Iteration 60, loss = 0.13929739\n",
      "Iteration 61, loss = 0.15878706\n",
      "Iteration 62, loss = 0.13723765\n",
      "Iteration 63, loss = 0.13487084\n",
      "Iteration 64, loss = 0.13218569\n",
      "Iteration 65, loss = 0.12927970\n",
      "Iteration 66, loss = 0.12956263\n",
      "Iteration 67, loss = 0.12771969\n",
      "Iteration 68, loss = 0.12428960\n",
      "Iteration 69, loss = 0.12626779\n",
      "Iteration 70, loss = 0.12072175\n",
      "Iteration 71, loss = 0.12081635\n",
      "Iteration 72, loss = 0.11964513\n",
      "Iteration 73, loss = 0.11663465\n",
      "Iteration 74, loss = 0.11810490\n",
      "Iteration 75, loss = 0.11317812\n",
      "Iteration 76, loss = 0.11069136\n",
      "Iteration 77, loss = 0.11049489\n",
      "Iteration 78, loss = 0.10689321\n",
      "Iteration 79, loss = 0.10897898\n",
      "Iteration 80, loss = 0.10698562\n",
      "Iteration 81, loss = 0.10427591\n",
      "Iteration 82, loss = 0.10640688\n",
      "Iteration 83, loss = 0.20506357\n",
      "Iteration 84, loss = 0.11997022\n",
      "Iteration 85, loss = 0.11301710\n",
      "Iteration 86, loss = 0.10933902\n",
      "Iteration 87, loss = 0.10731375\n",
      "Iteration 88, loss = 0.10816421\n",
      "Iteration 89, loss = 0.11254542\n",
      "Iteration 90, loss = 0.10275030\n",
      "Iteration 91, loss = 0.10148887\n",
      "Iteration 92, loss = 0.10338356\n",
      "Iteration 93, loss = 0.09321137\n",
      "Iteration 94, loss = 0.09931267\n",
      "Iteration 95, loss = 0.09541455\n",
      "Iteration 96, loss = 0.09583098\n",
      "Iteration 97, loss = 0.09400644\n",
      "Iteration 98, loss = 0.08779044\n",
      "Iteration 99, loss = 0.09317810\n",
      "Iteration 100, loss = 0.09168122\n",
      "Iteration 101, loss = 0.08809695\n",
      "Iteration 102, loss = 0.09263971\n",
      "Iteration 103, loss = 0.08838487\n",
      "Iteration 104, loss = 0.09179254\n",
      "Iteration 105, loss = 0.08338025\n",
      "Iteration 106, loss = 0.08357855\n",
      "Iteration 107, loss = 0.08460511\n",
      "Iteration 108, loss = 0.08844368\n",
      "Iteration 109, loss = 0.07820354\n",
      "Iteration 110, loss = 0.08192901\n",
      "Iteration 111, loss = 0.07683182\n",
      "Iteration 112, loss = 0.07333161\n",
      "Iteration 113, loss = 0.08652995\n",
      "Iteration 114, loss = 0.07772395\n",
      "Iteration 115, loss = 0.08060490\n",
      "Iteration 116, loss = 0.07830810\n",
      "Iteration 117, loss = 0.07403726\n",
      "Iteration 118, loss = 0.07635581\n",
      "Iteration 119, loss = 0.06984317\n",
      "Iteration 120, loss = 0.07185084\n",
      "Iteration 121, loss = 0.07358382\n",
      "Iteration 122, loss = 0.07506307\n",
      "Iteration 123, loss = 0.07734696\n",
      "Iteration 124, loss = 0.06500627\n",
      "Iteration 125, loss = 0.06728608\n",
      "Iteration 126, loss = 0.07359760\n",
      "Iteration 127, loss = 0.06576724\n",
      "Iteration 128, loss = 0.06979306\n",
      "Iteration 129, loss = 0.06668995\n",
      "Iteration 130, loss = 0.10020542\n",
      "Iteration 131, loss = 0.06481092\n",
      "Iteration 132, loss = 0.05989001\n",
      "Iteration 133, loss = 0.06176801\n",
      "Iteration 134, loss = 0.06386701\n",
      "Iteration 135, loss = 0.07014719\n",
      "Iteration 136, loss = 0.05920074\n",
      "Iteration 137, loss = 0.06445293\n",
      "Iteration 138, loss = 0.06498301\n",
      "Iteration 139, loss = 0.06624428\n",
      "Iteration 140, loss = 0.05876298\n",
      "Iteration 141, loss = 0.06709473\n",
      "Iteration 142, loss = 0.05742456\n",
      "Iteration 143, loss = 0.06036983\n",
      "Iteration 144, loss = 0.05932193\n",
      "Iteration 145, loss = 0.05767055\n",
      "Iteration 146, loss = 0.05439118\n",
      "Iteration 147, loss = 0.06561248\n",
      "Iteration 148, loss = 0.06279557\n",
      "Iteration 149, loss = 0.05523673\n",
      "Iteration 150, loss = 0.05885224\n",
      "Iteration 151, loss = 0.06541224\n",
      "Iteration 152, loss = 0.05154645\n",
      "Iteration 153, loss = 0.05580038\n",
      "Iteration 154, loss = 0.06447243\n",
      "Iteration 155, loss = 0.21886176\n",
      "Iteration 156, loss = 0.06575184\n",
      "Iteration 157, loss = 0.05882079\n",
      "Iteration 158, loss = 0.06918992\n",
      "Iteration 159, loss = 0.04998429\n",
      "Iteration 160, loss = 0.04860178\n",
      "Iteration 161, loss = 0.04794928\n",
      "Iteration 162, loss = 0.05541851\n",
      "Iteration 163, loss = 0.05005876\n",
      "Iteration 164, loss = 0.05615440\n",
      "Iteration 165, loss = 0.04942363\n",
      "Iteration 166, loss = 0.05142478\n",
      "Iteration 167, loss = 0.05237311\n",
      "Iteration 168, loss = 0.05373565\n",
      "Iteration 169, loss = 0.05399029\n",
      "Iteration 170, loss = 0.05164260\n",
      "Iteration 171, loss = 0.04805747\n",
      "Iteration 172, loss = 0.05413796\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(batch_size=50, hidden_layer_sizes=(128,), verbose=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf1 = MLPClassifier(\n",
    "    hidden_layer_sizes = (128,),\n",
    "    batch_size = 50,\n",
    "    solver = 'adam',\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "clf1.fit(v1, c1_old['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.29521352\n",
      "Iteration 2, loss = 0.23646435\n",
      "Iteration 3, loss = 0.21727416\n",
      "Iteration 4, loss = 0.21027434\n",
      "Iteration 5, loss = 0.19552299\n",
      "Iteration 6, loss = 0.19123976\n",
      "Iteration 7, loss = 0.18425600\n",
      "Iteration 8, loss = 0.17794381\n",
      "Iteration 9, loss = 0.17570567\n",
      "Iteration 10, loss = 0.17132961\n",
      "Iteration 11, loss = 0.16588510\n",
      "Iteration 12, loss = 0.16168129\n",
      "Iteration 13, loss = 0.16047170\n",
      "Iteration 14, loss = 0.15456410\n",
      "Iteration 15, loss = 0.15122162\n",
      "Iteration 16, loss = 0.14713748\n",
      "Iteration 17, loss = 0.14600203\n",
      "Iteration 18, loss = 0.14130635\n",
      "Iteration 19, loss = 0.14005184\n",
      "Iteration 20, loss = 0.13702810\n",
      "Iteration 21, loss = 0.13365677\n",
      "Iteration 22, loss = 0.13166521\n",
      "Iteration 23, loss = 0.12912677\n",
      "Iteration 24, loss = 0.12408592\n",
      "Iteration 25, loss = 0.12340024\n",
      "Iteration 26, loss = 0.12362981\n",
      "Iteration 27, loss = 0.11717177\n",
      "Iteration 28, loss = 0.11821284\n",
      "Iteration 29, loss = 0.11102938\n",
      "Iteration 30, loss = 0.11153758\n",
      "Iteration 31, loss = 0.10972422\n",
      "Iteration 32, loss = 0.10655424\n",
      "Iteration 33, loss = 0.10601628\n",
      "Iteration 34, loss = 0.10010331\n",
      "Iteration 35, loss = 0.09924234\n",
      "Iteration 36, loss = 0.09653122\n",
      "Iteration 37, loss = 0.09569788\n",
      "Iteration 38, loss = 0.09191739\n",
      "Iteration 39, loss = 0.09152131\n",
      "Iteration 40, loss = 0.09035071\n",
      "Iteration 41, loss = 0.08639525\n",
      "Iteration 42, loss = 0.08633114\n",
      "Iteration 43, loss = 0.08181057\n",
      "Iteration 44, loss = 0.08278807\n",
      "Iteration 45, loss = 0.07737445\n",
      "Iteration 46, loss = 0.07597112\n",
      "Iteration 47, loss = 0.07689880\n",
      "Iteration 48, loss = 0.07340820\n",
      "Iteration 49, loss = 0.07180279\n",
      "Iteration 50, loss = 0.06958074\n",
      "Iteration 51, loss = 0.07040520\n",
      "Iteration 52, loss = 0.06488445\n",
      "Iteration 53, loss = 0.06865573\n",
      "Iteration 54, loss = 0.06594995\n",
      "Iteration 55, loss = 0.06296778\n",
      "Iteration 56, loss = 0.06686593\n",
      "Iteration 57, loss = 0.06102159\n",
      "Iteration 58, loss = 0.05758783\n",
      "Iteration 59, loss = 0.05689417\n",
      "Iteration 60, loss = 0.05640093\n",
      "Iteration 61, loss = 0.05645810\n",
      "Iteration 62, loss = 0.05643318\n",
      "Iteration 63, loss = 0.05591862\n",
      "Iteration 64, loss = 0.05208865\n",
      "Iteration 65, loss = 0.05200022\n",
      "Iteration 66, loss = 0.05066522\n",
      "Iteration 67, loss = 0.05026630\n",
      "Iteration 68, loss = 0.05303804\n",
      "Iteration 69, loss = 0.04619679\n",
      "Iteration 70, loss = 0.05185530\n",
      "Iteration 71, loss = 0.04451152\n",
      "Iteration 72, loss = 0.04534823\n",
      "Iteration 73, loss = 0.04749922\n",
      "Iteration 74, loss = 0.04497726\n",
      "Iteration 75, loss = 0.04391506\n",
      "Iteration 76, loss = 0.04083495\n",
      "Iteration 77, loss = 0.04478673\n",
      "Iteration 78, loss = 0.05279340\n",
      "Iteration 79, loss = 0.04630240\n",
      "Iteration 80, loss = 0.03933733\n",
      "Iteration 81, loss = 0.03644952\n",
      "Iteration 82, loss = 0.03968634\n",
      "Iteration 83, loss = 0.04286015\n",
      "Iteration 84, loss = 0.04025902\n",
      "Iteration 85, loss = 0.04156890\n",
      "Iteration 86, loss = 0.03904383\n",
      "Iteration 87, loss = 0.03776795\n",
      "Iteration 88, loss = 0.03768162\n",
      "Iteration 89, loss = 0.04144108\n",
      "Iteration 90, loss = 0.03506871\n",
      "Iteration 91, loss = 0.03547816\n",
      "Iteration 92, loss = 0.03266061\n",
      "Iteration 93, loss = 0.04149640\n",
      "Iteration 94, loss = 0.03475943\n",
      "Iteration 95, loss = 0.03631276\n",
      "Iteration 96, loss = 0.04302132\n",
      "Iteration 97, loss = 0.03809486\n",
      "Iteration 98, loss = 0.03186283\n",
      "Iteration 99, loss = 0.03540369\n",
      "Iteration 100, loss = 0.03179976\n",
      "Iteration 101, loss = 0.02977423\n",
      "Iteration 102, loss = 0.03167814\n",
      "Iteration 103, loss = 0.04022558\n",
      "Iteration 104, loss = 0.03185103\n",
      "Iteration 105, loss = 0.03533294\n",
      "Iteration 106, loss = 0.02888846\n",
      "Iteration 107, loss = 0.02793482\n",
      "Iteration 108, loss = 0.02808494\n",
      "Iteration 109, loss = 0.03346009\n",
      "Iteration 110, loss = 0.03487626\n",
      "Iteration 111, loss = 0.03683152\n",
      "Iteration 112, loss = 0.03325705\n",
      "Iteration 113, loss = 0.03037755\n",
      "Iteration 114, loss = 0.03487534\n",
      "Iteration 115, loss = 0.02955108\n",
      "Iteration 116, loss = 0.03024829\n",
      "Iteration 117, loss = 0.03708190\n",
      "Iteration 118, loss = 0.03247268\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(batch_size=50, hidden_layer_sizes=(128,), verbose=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = MLPClassifier(\n",
    "    hidden_layer_sizes = (128,),\n",
    "    batch_size = 50,\n",
    "    solver = 'adam',\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "clf2.fit(v2, c2_old['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2_old['bert_proba'] = clf1.predict_proba(v2)[:,1]\n",
    "c1_old['bert_proba'] = clf2.predict_proba(v1)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_c1_old = c1_old[~pd.isnull(c1_old['text_y_cleaned'])]\n",
    "_c2_old = c2_old[~pd.isnull(c2_old['text_y_cleaned'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 200)\n",
    "# c1[['page','crop_diff','font_diff3','footer_diff','header_diff','label']][:200]\n",
    "# c1[c1['font_diff3'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on c1, predict c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Vectors only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size    68.910383\n",
      "P        0.684255\n",
      "R        0.693042\n",
      "F1       0.537675\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X_train = v1\n",
    "y_train = c1_old['label']\n",
    "X_test = v2\n",
    "y_test = c2_old['label']\n",
    "\n",
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes = (128,),\n",
    "    batch_size = 50,\n",
    "    solver = 'adam',\n",
    "    verbose = 0\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "true = y_test\n",
    "preds = clf.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(clf.predict(X_test))\n",
    "print(MeanBcubed(vb_truth,vb_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual + Bert_similarity i-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size    68.910383\n",
      "P        0.434505\n",
      "R        0.833364\n",
      "F1       0.355291\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "features = ['font_diff3','crop_diff','bert_sim']\n",
    "\n",
    "X_train = c1_old[features]\n",
    "y_train = c1_old['label']\n",
    "X_test = c2_old[features]\n",
    "y_test = c2_old['label']\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "true = y_test\n",
    "preds = model.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(model.predict(X_test))\n",
    "print(MeanBcubed(vb_truth,vb_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual + Bert_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size    68.910383\n",
      "P        0.593553\n",
      "R        0.724658\n",
      "F1       0.431767\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "features = ['font_diff3','crop_diff','bert_proba']\n",
    "\n",
    "X_train = c1_old[features]\n",
    "y_train = c1_old['label']\n",
    "X_test = c2_old[features]\n",
    "y_test = c2_old['label']\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "true = y_test\n",
    "preds = model.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(model.predict(X_test))\n",
    "print(MeanBcubed(vb_truth,vb_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on c2, predict c1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert vectors only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size    34.380975\n",
      "P        0.648183\n",
      "R        0.725552\n",
      "F1       0.537412\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# features = ['font_diff2','crop_diff','text_d2v_sim_score']\n",
    "X_train = v2\n",
    "y_train = c2_old['label']\n",
    "X_test = v1\n",
    "y_test = c1_old['label']\n",
    "\n",
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes = (128,),\n",
    "    batch_size = 50,\n",
    "    solver = 'adam',\n",
    "    verbose = 0\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "true = y_test\n",
    "preds = clf.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(clf.predict(X_test))\n",
    "print(MeanBcubed(vb_truth,vb_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual + Bert_similarity i-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size    34.380975\n",
      "P        0.150656\n",
      "R        0.962881\n",
      "F1       0.183027\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "features = ['font_diff3','crop_diff','bert_sim']\n",
    "\n",
    "X_train = c2_old[features]\n",
    "y_train = c2_old['label']\n",
    "X_test = c1_old[features]\n",
    "y_test = c1_old['label']\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "true = y_test\n",
    "preds = clf.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(clf.predict(X_test))\n",
    "print(MeanBcubed(vb_truth,vb_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual + Bert_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size    34.380975\n",
      "P        0.350439\n",
      "R        0.963462\n",
      "F1       0.420291\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "features = ['font_diff3','crop_diff','bert_proba']\n",
    "\n",
    "X_train = c2_old[features]\n",
    "y_train = c2_old['label']\n",
    "X_test = c1_old[features]\n",
    "y_test = c1_old['label']\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "true = y_test\n",
    "preds = clf.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(clf.predict(X_test))\n",
    "print(MeanBcubed(vb_truth,vb_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP - BERT + Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19101/19101 [00:03<00:00, 5043.83it/s]\n",
      "100%|██████████| 16537/16537 [00:03<00:00, 5085.18it/s]\n"
     ]
    }
   ],
   "source": [
    "v1_plus = [np.append(v1[i], [c1_old.iloc[i]['font_diff3'], c1_old.iloc[i]['crop_diff']]) for i in tqdm(range(len(v1)))]\n",
    "v2_plus = [np.append(v2[i], [c2_old.iloc[i]['font_diff3'], c2_old.iloc[i]['crop_diff']]) for i in tqdm(range(len(v2)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train C1, Predict C2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size    68.910383\n",
      "P        0.633823\n",
      "R        0.720345\n",
      "F1       0.498019\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# features = ['font_diff2','crop_diff','text_d2v_sim_score']\n",
    "X_train = v1_plus\n",
    "y_train = c1_old['label']\n",
    "X_test = v2_plus\n",
    "y_test = c2_old['label']\n",
    "\n",
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes = (128,),\n",
    "    batch_size = 50,\n",
    "    solver = 'adam',\n",
    "    verbose = 0\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "true = y_test\n",
    "preds = clf.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(clf.predict(X_test))\n",
    "print(MeanBcubed(vb_truth,vb_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train C2, Predict C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size    34.380975\n",
      "P        0.783197\n",
      "R        0.715920\n",
      "F1       0.631302\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# features = ['font_diff2','crop_diff','text_d2v_sim_score']\n",
    "X_train = v2_plus\n",
    "y_train = c2_old['label']\n",
    "X_test = v1_plus\n",
    "y_test = c1_old['label']\n",
    "\n",
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes = (128,),\n",
    "    batch_size = 50,\n",
    "    solver = 'adam',\n",
    "    verbose = 0\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "true = y_test\n",
    "preds = clf.predict(X_test)\n",
    "vb_truth, vb_pred = change_format(y_test.values), change_format(clf.predict(X_test))\n",
    "print(MeanBcubed(vb_truth,vb_pred))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0ef08d8ed6cc4ea55e1d44bd248b5018e6b6290053252328e46dd8d04768404"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('thesis8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
